<!doctype html><html lang=en><head><title>hodoop集群搭建之分发脚本以及ssh免密登录 · YanHowey Wang</title><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=color-scheme content="light dark"><meta name=author content="YanHowey Wang"><meta name=description content="hodoop 介绍 Link to heading @TOC
一、hadoop 三种运行模式 Link to heading 本地模式 ：数据存储在 linux 本地（只是在测试偶尔用） 伪分布式：数据存储在 HDFS（一台主机模拟多主机） 完全分布式：数据存储在 HDFS（多台服务器工作）企业大量使用
二、模式示例 Link to heading 1.本地模式 Link to heading 在 hadoop 文件夹下新建
mkdir wcinput cd wcinput vim word.txt //任意写入内容 ./wcoutput 要是不存在的文件夹，不然会抛出异常
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount wcinput/ ./wcoutput cat part-r-00000 2.完全分布式模式 Link to heading （1）流程 Link to heading 准备 3 台客户机（关闭防火墙，静态 ip，主机名称） List item 安装 jdk，hadoop 配置环境变量 配置集群 单点启动 配置 ssh 群起测试集群 （2）编写集群分发脚本 xsync Link to heading 1."><meta name=keywords content="blog,编程,前端,后端"><meta name=twitter:card content="summary"><meta name=twitter:title content="hodoop集群搭建之分发脚本以及ssh免密登录"><meta name=twitter:description content="hodoop 介绍 Link to heading @TOC
一、hadoop 三种运行模式 Link to heading 本地模式 ：数据存储在 linux 本地（只是在测试偶尔用） 伪分布式：数据存储在 HDFS（一台主机模拟多主机） 完全分布式：数据存储在 HDFS（多台服务器工作）企业大量使用
二、模式示例 Link to heading 1.本地模式 Link to heading 在 hadoop 文件夹下新建
mkdir wcinput cd wcinput vim word.txt //任意写入内容 ./wcoutput 要是不存在的文件夹，不然会抛出异常
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount wcinput/ ./wcoutput cat part-r-00000 2.完全分布式模式 Link to heading （1）流程 Link to heading 准备 3 台客户机（关闭防火墙，静态 ip，主机名称） List item 安装 jdk，hadoop 配置环境变量 配置集群 单点启动 配置 ssh 群起测试集群 （2）编写集群分发脚本 xsync Link to heading 1."><meta property="og:title" content="hodoop集群搭建之分发脚本以及ssh免密登录"><meta property="og:description" content="hodoop 介绍 Link to heading @TOC
一、hadoop 三种运行模式 Link to heading 本地模式 ：数据存储在 linux 本地（只是在测试偶尔用） 伪分布式：数据存储在 HDFS（一台主机模拟多主机） 完全分布式：数据存储在 HDFS（多台服务器工作）企业大量使用
二、模式示例 Link to heading 1.本地模式 Link to heading 在 hadoop 文件夹下新建
mkdir wcinput cd wcinput vim word.txt //任意写入内容 ./wcoutput 要是不存在的文件夹，不然会抛出异常
hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount wcinput/ ./wcoutput cat part-r-00000 2.完全分布式模式 Link to heading （1）流程 Link to heading 准备 3 台客户机（关闭防火墙，静态 ip，主机名称） List item 安装 jdk，hadoop 配置环境变量 配置集群 单点启动 配置 ssh 群起测试集群 （2）编写集群分发脚本 xsync Link to heading 1."><meta property="og:type" content="article"><meta property="og:url" content="https://howey.deno.dev/posts/2307262209/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2023-07-26T22:21:16+08:00"><meta property="article:modified_time" content="2023-07-26T22:21:16+08:00"><link rel=canonical href=https://howey.deno.dev/posts/2307262209/><link rel=preload href="/fonts/forkawesome-webfont.woff2?v=1.2.0" as=font type=font/woff2 crossorigin><link rel=stylesheet href=/css/coder.min.5adbe72fc41dcfb852215b84695288939b6b606db73238bd3ee936469572fc9c.css integrity="sha256-WtvnL8Qdz7hSIVuEaVKIk5trYG23Mji9Puk2RpVy/Jw=" crossorigin=anonymous media=screen><link rel=stylesheet href=/css/coder-dark.min.a00e6364bacbc8266ad1cc81230774a1397198f8cfb7bcba29b7d6fcb54ce57f.css integrity="sha256-oA5jZLrLyCZq0cyBIwd0oTlxmPjPt7y6KbfW/LVM5X8=" crossorigin=anonymous media=screen><link rel=icon type=image/svg+xml href=/images/favicon.svg sizes=any><link rel=icon type=image/png href=/images/favicon-32x32.png sizes=32x32><link rel=icon type=image/png href=/images/favicon-16x16.png sizes=16x16><link rel=apple-touch-icon href=/images/apple-touch-icon.png><link rel=apple-touch-icon sizes=180x180 href=/images/apple-touch-icon.png><link rel=manifest href=/site.webmanifest><link rel=mask-icon href=/images/safari-pinned-tab.svg color=#5bbad5></head><body class="preload-transitions colorscheme-auto"><div class=float-container><a id=dark-mode-toggle class=colorscheme-toggle><i class="fa fa-adjust fa-fw" aria-hidden=true></i></a></div><main class=wrapper><nav class=navigation><section class=container><a class=navigation-title href=/>YanHowey Wang</a>
<input type=checkbox id=menu-toggle>
<label class="menu-button float-right" for=menu-toggle><i class="fa fa-bars fa-fw" aria-hidden=true></i></label><ul class=navigation-list><li class=navigation-item><a class=navigation-link href=/categories/>Categories</a></li><li class=navigation-item><a class=navigation-link href=/tags/>Tags</a></li><li class=navigation-item><a class=navigation-link href=/posts/>Blog</a></li><li class=navigation-item><a class=navigation-link href=/contact/>Contact me</a></li><li class=navigation-item><a class=navigation-link href=/about/>About</a></li></ul></section></nav><div class=content><section class="container post"><article><header><div class=post-title><h1 class=title><a class=title-link href=https://howey.deno.dev/posts/2307262209/>hodoop集群搭建之分发脚本以及ssh免密登录</a></h1></div><div class=post-meta><div class=date><span class=posted-on><i class="fa fa-calendar" aria-hidden=true></i>
<time datetime=2023-07-26T22:21:16+08:00>July 26, 2023</time></span>
<span class=reading-time><i class="fa fa-clock-o" aria-hidden=true></i>
2-minute read</span></div><div class=categories><i class="fa fa-folder" aria-hidden=true></i>
<a href=/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/>大数据</a></div><div class=tags><i class="fa fa-tag" aria-hidden=true></i>
<span class=tag><a href=/tags/hadoop/>hadoop</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/ssh/>ssh</a></span>
<span class=separator>•</span>
<span class=tag><a href=/tags/shell/>shell</a></span></div></div></header><div class=post-content><h1 id=hodoop-介绍>hodoop 介绍
<a class=heading-link href=#hodoop-%e4%bb%8b%e7%bb%8d><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><hr><p>@<a href=%e6%96%87%e7%ab%a0%e7%9b%ae%e5%bd%95>TOC</a></p><hr><h1 id=一hadoop-三种运行模式>一、hadoop 三种运行模式
<a class=heading-link href=#%e4%b8%80hadoop-%e4%b8%89%e7%a7%8d%e8%bf%90%e8%a1%8c%e6%a8%a1%e5%bc%8f><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><blockquote><p>本地模式 ：数据存储在 linux 本地（只是在测试偶尔用）
伪分布式：数据存储在 HDFS（一台主机模拟多主机）
完全分布式：数据存储在 HDFS（多台服务器工作）企业大量使用</p></blockquote><h1 id=二模式示例>二、模式示例
<a class=heading-link href=#%e4%ba%8c%e6%a8%a1%e5%bc%8f%e7%a4%ba%e4%be%8b><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h1><h2 id=1本地模式>1.本地模式
<a class=heading-link href=#1%e6%9c%ac%e5%9c%b0%e6%a8%a1%e5%bc%8f><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><blockquote><p>在 hadoop 文件夹下新建</p></blockquote><pre tabindex=0><code class=language-linux data-lang=linux>mkdir wcinput
cd wcinput
vim word.txt  //任意写入内容
</code></pre><p>./wcoutput 要是不存在的文件夹，不然会抛出异常</p><pre tabindex=0><code class=language-linux data-lang=linux>hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-3.3.1.jar wordcount wcinput/ ./wcoutput
cat part-r-00000
</code></pre><h2 id=2完全分布式模式>2.完全分布式模式
<a class=heading-link href=#2%e5%ae%8c%e5%85%a8%e5%88%86%e5%b8%83%e5%bc%8f%e6%a8%a1%e5%bc%8f><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h2><h3 id=1流程>（1）流程
<a class=heading-link href=#1%e6%b5%81%e7%a8%8b><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><ol><li>准备 3 台客户机（关闭防火墙，静态 ip，主机名称）</li><li>List item</li><li>安装 jdk，hadoop</li><li>配置环境变量</li><li>配置集群</li><li>单点启动</li><li>配置 ssh</li><li>群起测试集群</li></ol><h3 id=2编写集群分发脚本-xsync>（2）编写集群分发脚本 xsync
<a class=heading-link href=#2%e7%bc%96%e5%86%99%e9%9b%86%e7%be%a4%e5%88%86%e5%8f%91%e8%84%9a%e6%9c%ac-xsync><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><h4 id=1scp-安全拷贝>1.scp 安全拷贝
<a class=heading-link href=#1scp-%e5%ae%89%e5%85%a8%e6%8b%b7%e8%b4%9d><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><ol><li>scp 定义：
scp 可以实现服务器与服务器之间的数据拷贝</li><li>基本语法：
scp -r 要拷贝的文件路径/名称 目的地用户名@主机:目的地路径名称
-r 表示递归
例子：102 推送至 103 <code>scp -r jdk1.8.0_331/ root@hadoop103:/opt/module/</code>
103 从 102 拉取：<code>scp -r root@hadoop102:/opt/module/hadoop-3.3.1 ./</code>
103 拷贝 102 到 104：
<code>scp -r root@hadoop102:/opt/module/* root@hadoop104:/opt/module/</code></li></ol><h4 id=2rsync-远程同步工具>2.rsync 远程同步工具
<a class=heading-link href=#2rsync-%e8%bf%9c%e7%a8%8b%e5%90%8c%e6%ad%a5%e5%b7%a5%e5%85%b7><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>rsync 主要用于备份和镜像。具有速度快、避免复制相同内容和支持符号链接的优点
rsync 和 scp 的区别：用 rsync 做文件的复制要比 scp 的速度快，rsync 只对差异文件做更新。scp 是把所有文件都复制过去
（1）基本语法
rsync -av 要拷贝的文件路径/名称 目的地用户名@主机:目的地路径名称
-a 表示归档拷贝
-v 显示复制过程
示例：<code>rsync -av hadoop-3.3.1/ root@hadoop103:/opt/module/</code></p><h4 id=3集群分发脚本-xsync>3.集群分发脚本 xsync
<a class=heading-link href=#3%e9%9b%86%e7%be%a4%e5%88%86%e5%8f%91%e8%84%9a%e6%9c%ac-xsync><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><div class=highlight><pre tabindex=0 class=chroma><code class=language-shell data-lang=shell><span class=line><span class=cl><span class=cp>#!/bin/bash
</span></span></span><span class=line><span class=cl><span class=cp></span>
</span></span><span class=line><span class=cl><span class=c1>#1.判断参数个数</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>if</span> <span class=o>[</span> <span class=nv>$#</span> -lt <span class=m>1</span> <span class=o>]</span>
</span></span><span class=line><span class=cl>	<span class=k>then</span>
</span></span><span class=line><span class=cl>		<span class=nb>echo</span> not enough argument!
</span></span><span class=line><span class=cl>	exit<span class=p>;</span>
</span></span><span class=line><span class=cl><span class=k>fi</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=c1>#2.便利集群所有机器</span>
</span></span><span class=line><span class=cl><span class=k>for</span> host in hadoop102 hadoop103 hadoop104
</span></span><span class=line><span class=cl><span class=k>do</span>
</span></span><span class=line><span class=cl>	<span class=nb>echo</span> <span class=o>===========================================</span><span class=nv>$host</span><span class=o>==========================</span>
</span></span><span class=line><span class=cl>	<span class=c1>#3.遍历所有目录</span>
</span></span><span class=line><span class=cl>	<span class=k>for</span> file in <span class=nv>$@</span>
</span></span><span class=line><span class=cl>	<span class=k>do</span>
</span></span><span class=line><span class=cl>		<span class=c1>#4.判断文件是否存在</span>
</span></span><span class=line><span class=cl>		<span class=k>if</span> <span class=o>[</span> -e <span class=nv>$file</span> <span class=o>]</span>
</span></span><span class=line><span class=cl>			<span class=k>then</span>
</span></span><span class=line><span class=cl>				<span class=c1>#5.获取父目录(软连接的处理)</span>
</span></span><span class=line><span class=cl>				<span class=nv>pdir</span><span class=o>=</span><span class=k>$(</span><span class=nb>cd</span> -P <span class=k>$(</span>dirname <span class=nv>$file</span><span class=k>)</span><span class=p>;</span> <span class=nb>pwd</span><span class=k>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl>				<span class=c1>#6.获取当前文件的名称</span>
</span></span><span class=line><span class=cl>				<span class=nv>fname</span><span class=o>=</span><span class=k>$(</span>basename <span class=nv>$file</span><span class=k>)</span>
</span></span><span class=line><span class=cl>				ssh <span class=nv>$host</span> <span class=s2>&#34;mkdir -p </span><span class=nv>$pdir</span><span class=s2>&#34;</span>
</span></span><span class=line><span class=cl>				rsync -av <span class=nv>$pdir</span>/<span class=nv>$fname</span> <span class=nv>$host</span>:<span class=nv>$pdir</span>
</span></span><span class=line><span class=cl>			<span class=k>else</span>
</span></span><span class=line><span class=cl>				<span class=nb>echo</span> <span class=nv>$file</span> does not exists!
</span></span><span class=line><span class=cl>		<span class=k>fi</span>
</span></span><span class=line><span class=cl>	<span class=k>done</span>
</span></span><span class=line><span class=cl><span class=k>done</span>
</span></span></code></pre></div><p>给权限执行
<code>chmod 777 xsync</code>
拷贝全局可用
<code>sudo cp /home/howey/bin/xsync /bin/</code>
执行示例
<code>xsync a.txt</code></p><h4 id=4ssh-免密登录切换用户需要再次执行>4.ssh 免密登录（切换用户需要再次执行）
<a class=heading-link href=#4ssh-%e5%85%8d%e5%af%86%e7%99%bb%e5%bd%95%e5%88%87%e6%8d%a2%e7%94%a8%e6%88%b7%e9%9c%80%e8%a6%81%e5%86%8d%e6%ac%a1%e6%89%a7%e8%a1%8c><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h4><p>生成密钥
<code>ssh-keygen -t rsa</code>
将本地的 ssh 公钥文件安装到远程主机对应的账户下
<code>ssh-copy-id hadoop103</code>
免密原理：
<strong>A 服务器：生成密钥对，将 A 公钥安装到 B 服务器下，
当 Assh 访问 B 时携带经过私钥 A 加密的数据，B 服务器接收到之后，去授权 key 查找到 A 的公钥，根据服务器 A 的公钥进行解密。
返回的数据采用 A 的公钥进行加密返回给 A，A 接收返回的数据采用私钥 A 进行解密。</strong></p></div><footer><section class=see-also><h3 id=see-also-in-编程系列>See also in 编程系列
<a class=heading-link href=#see-also-in-%e7%bc%96%e7%a8%8b%e7%b3%bb%e5%88%97><i class="fa fa-link" aria-hidden=true title="Link to heading"></i>
<span class=sr-only>Link to heading</span></a></h3><nav><ul><li><a href=/posts/2308061943/>前端vue小知识</a></li><li><a href=/posts/2308052141/>windows如何取消桌面图标右下角带快捷方式？</a></li><li><a href=/posts/23080521/>如何完全卸载chrome？</a></li><li><a href=/posts/230805/>如何获取快手直播弹幕？</a></li><li><a href=/posts/2307262210/>hugo + github actions + deno deploy部署自己的博客项目</a></li><li><a href=/posts/2307262202/>gin 项目初始化之mysql连接</a></li></ul></nav></section></footer></article></section></div><footer class=footer><section class=container>©
2019 -
2023
YanHowey Wang
·
Powered by <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a> & <a href=https://github.com/luizdepra/hugo-coder/ target=_blank rel=noopener>Coder</a>.</section></footer></main><script src=/js/coder.min.6ae284be93d2d19dad1f02b0039508d9aab3180a12a06dcc71b0b0ef7825a317.js integrity="sha256-auKEvpPS0Z2tHwKwA5UI2aqzGAoSoG3McbCw73gloxc="></script></body></html>